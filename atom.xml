<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>XuefeiLi(李雪霏)</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2021-09-25T10:33:06.704Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Xuefei Li</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Introduction</title>
    <link href="http://yoursite.com/2019/11/07/intro/"/>
    <id>http://yoursite.com/2019/11/07/intro/</id>
    <published>2019-11-08T02:07:14.000Z</published>
    <updated>2021-09-25T10:33:06.704Z</updated>
    
    <content type="html"><![CDATA[<!-- ![profile](/assets/profile.jpg) --><img src="/assets/profile.jpg" width="200" height="200" /><h3 id="Education"><a href="#Education" class="headerlink" title="Education"></a>Education</h3><ul><li><strong>College of Science and Engineering, University of Minnesota Twin cities</strong>, Sep 2018 - now<br>M.S. in Computer Science, Plan B project-based, GPA: 4.0/4.0</li><li><strong>School of Aeronautics and Astronautics, Fudan University</strong>, Sep 2014 - Jun 2018<br>B.S. in Theoretical and Applied Mechanics, ranking: 6/57</li><li><strong>School of Computer Science, Fudan University</strong>, Sep 2016 - Jun 2018<br>Minor in Data Science, GPA: 3.70/4.0</li><li><strong>Major courses:</strong> Computer Graphics (Grad, A), Visualization (Grad, A), Artificial Intelligence (Grad, A), Computer Vision (Grad, A), Intro to Data Mining (Grad, A), Machine Learning (Undergrad, A), Digital Signal Processing (Undergrad, A), Mathematical Modeling (Undergrad, A), Partial Differential Equation (Undergrad, A), Probability &amp; Mathematical Statistics (Undergrad, A), Advanced Algebra and Analytic Geometry (Undergrad, A-)</li></ul><h3 id="Professional-Experience"><a href="#Professional-Experience" class="headerlink" title="Professional Experience"></a>Professional Experience</h3><ul><li><strong>HP Inc. Internship as Computer Vision Engineer</strong>, Shanghai, China. Apr 2018 - Jul 2018<br>Use deep learning to compress images, and also decompress to preserve good quality.</li><li><strong>iQiyi.com Inc. Internship as Algorithms Engineer</strong>, Shanghai, China. Jan 2018 - Feb 2018<br>Designed the learning Question Answering over QA Corpora and Knowledge Bases developed.</li><li><strong>University of Illinois at Urbana-Champaign, as visiting scholar</strong>, with Professor Robert J. Brunner, Jun 2017 - Sep 2017<br>Implemented ConvNets for feature learning on Sloan Digital Sky Survey (SDSS) images, and designed a generative model with Hierarchical and Variational Autoencoder and unsupervised classification of stars and galaxies.</li></ul><h3 id="Research-Interest"><a href="#Research-Interest" class="headerlink" title="Research Interest"></a>Research Interest</h3><p>Visualization, Vision, Animation, Artificial Intelligence, Machine Learning, Computer Graphics, Physical-based Simulation</p><h3 id="Skills"><a href="#Skills" class="headerlink" title="Skills"></a>Skills</h3><ul><li>Computer/Technical skills:<br>C/C++, OpenGL, Python, Matlab, Processing, Javascript, AutoCAD, ANSYS, Photoshop…</li><li>Hobbies:<br>Piano(Level-9 certificated with Chinese Musician Association), Hiking, drawing…</li></ul><h3 id="Contact"><a href="#Contact" class="headerlink" title="Contact"></a>Contact</h3><p>Email: lixuefei9679 at gmail dot com</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;!-- ![profile](/assets/profile.jpg) --&gt;
&lt;img src=&quot;/assets/profile.jpg&quot; width=&quot;200&quot; height=&quot;200&quot; /&gt;

&lt;h3 id=&quot;Education&quot;&gt;&lt;a href=&quot;#Education&quot;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Mimic Robot</title>
    <link href="http://yoursite.com/2019/11/04/Mimic-Robot/"/>
    <id>http://yoursite.com/2019/11/04/Mimic-Robot/</id>
    <published>2019-11-05T01:15:27.000Z</published>
    <updated>2021-09-25T12:14:08.733Z</updated>
    
    <content type="html"><![CDATA[<p>This project is my Plan B project for M.S. degree at UMN, instructed by Prof. Stephen J. Guy. It is about data-driven character animation. The data I used is daily videos, instead of motion capture data, which is a more difficult task. First task is to generate pose estimation given a video clip, and transfer the pose to artificial character and reconstruct the 3d pose. Reinforcement learning is necessary in the process to generate plausible pose.</p><p><a href="https://docs.google.com/presentation/d/1cPaCvaVVI2ldeOwUuJHkFVUTCsOKkS-o/edit?usp=sharing&ouid=106807665038464439636&rtpof=true&sd=true" target="_blank" rel="noopener">Slides</a>, <a href="https://xuefei-lxf.github.io/pdf/report.pdf" target="_blank" rel="noopener">Report</a>, <a href="https://xuefei-lxf.github.io/video/mimic.mov" target="_blank" rel="noopener">Demo</a></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Physics-based simulation of passive phenomena of objects is common, while modeling motion of humans and animals is challenging.</p><p>Manually made controller could be stylized and engaging, however it has many drawbacks,<br>It doesn’t have generality;<br>It requiring considerable effort to construct, the range of motions is limited to the space of all possible reactions</p><h3 id="How-to-get-motions"><a href="#How-to-get-motions" class="headerlink" title="How to get motions?"></a>How to get motions?</h3><ol><li>Motion Capture</li><li>In-the-wild videos</li></ol><h3 id="Goal-train-physically-simulated-characters-to-imitating-reference-motion-from-daily-videos"><a href="#Goal-train-physically-simulated-characters-to-imitating-reference-motion-from-daily-videos" class="headerlink" title="Goal:  train physically simulated characters to imitating reference motion from daily videos"></a>Goal:  train physically simulated characters to imitating reference motion from daily videos</h3><p><img src="/assets/mimcrobot/overview.png" alt="overview"></p><h2 id="Pose-estimation"><a href="#Pose-estimation" class="headerlink" title="Pose estimation"></a>Pose estimation</h2><p>Use <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" target="_blank" rel="noopener">OpenPose</a> to generate 2D real-time multi-person keypoint detection.</p><p><img src="/assets/mimcrobot/mr1.png" alt="mr1"></p><p> Keypoints are stored in array and passed to another model from <a href="https://github.com/facebookresearch/VideoPose3D" target="_blank" rel="noopener">VideoPose3D</a> - 3D human pose estimation in video with temporal convolutions and semi-supervised training,</p><p><img src="/assets/mimcrobot/mr2.gif" alt="mr2"></p><h2 id="3D-reconstruction"><a href="#3D-reconstruction" class="headerlink" title="3D reconstruction"></a>3D reconstruction</h2><p>Chanllenges:</p><ul><li>Lack of largescale ground truth 3D annotation for in-the-wild images</li><li>Inherent ambiguities in single-view 2D-to-3D mapping</li><li>Depth ambiguity where multiple 3D body configurations explain the same 2D projections</li><li>…</li></ul><p>After extracting features from images, use a temporal generation network to preserve some temporal information, also apply adversarial learning  a large-scale MoCap dataset to generate kinematically plausible motion sequences.</p><!-- <p float="left"> --><p><img align="left" src="/assets/mimcrobot/pose1.gif" width="140" height="250"/><img align="left" src="/assets/mimcrobot/pose2.gif" width="450" height="250"/> <img align="rightt" src="/assets/mimcrobot/pose3.gif" width="250" height="250" /></p><!-- </p> --><h3 id="Pose-transfer"><a href="#Pose-transfer" class="headerlink" title="Pose transfer:"></a>Pose transfer:</h3><img src="/assets/mimcrobot/pose.png" width="700" height="400" /><!-- ![pt](/assets/mimcrobot/pose.png)  --><p>Sequences of poses ransformed into:<br>root position (3D), root rotation (4D), chest rotation (4D), neck rotation (4D), right hip rotation (4D), right knee rotation (1D), right ankle rotation (4D), right shoulder rotation (4D), right elbow rotation (1D), left hip rotation (4D), left knee rotation (1D), left ankle rotation (4D), left shoulder rotation (4D), left elbow rotation (1D) </p><h2 id="Motion-imitation"><a href="#Motion-imitation" class="headerlink" title="Motion imitation"></a>Motion imitation</h2><p>Simulation environment: PyBullet<br>State: relative positions, rotations, linear and angular velocities of each link with respect to the root<br>Action:  target orientations for PD controllers at each joint<br>Policy determines which actions should be applied at each timestep in order to reproduce the desired motion </p><p><img src="/assets/mimcrobot/test1.gif" alt="t1"> <img src="/assets/mimcrobot/test2.gif" alt="t2"><br>Test on mocap data (up) and in-the-wild video (down) after training with 8 workers for 1 day.</p><!-- While simply pass the generated pose to the character failed, because the body configurations are different, and that the generated 3D poses are neither stable nor reliable. Therefore we will implement Reinforcement Learning to predict poses for each frame, and sequence them into a trajectory to generate a consistant motion .![mr4](/assets/mimcrobot/mr4.png)![mr5](/assets/mimcrobot/mr6.gif)This part will be completed by this semester. -->]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This project is my Plan B project for M.S. degree at UMN, instructed by Prof. Stephen J. Guy. It is about data-driven character animation
      
    
    </summary>
    
    
    
      <category term="Character animation, Pose Estimation, Reinforcement Learning" scheme="http://yoursite.com/tags/Character-animation-Pose-Estimation-Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Egocentric Cognitive Mapping</title>
    <link href="http://yoursite.com/2019/11/03/eco/"/>
    <id>http://yoursite.com/2019/11/03/eco/</id>
    <published>2019-11-04T01:00:39.000Z</published>
    <updated>2021-09-25T07:45:53.837Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Trying to build a cognitive map from First-Person videos that can be used to understand <em>novel</em> but <em>similar</em> environments, which will benefit visually-impaired people. Instructed by Prof. Hyun Soo Park and his group at UMN.</p><p><img src="/assets/eco/eco.gif" alt="eco"></p><p><a href="https://youtu.be/3wsUlLbtzko" target="_blank" rel="noopener">Demo</a>: Groccery store data annotation with ECO;<br><a href="https://github.com/semantic-localization" target="_blank" rel="noopener">Code</a></p><p>Local Egocentric Maps</p><p><img src="/assets/eco/eco1.jpg" alt="eco1">)<img src="/assets/eco/eco2.jpg" alt="eco2"></p><p>Local Egocentric Maps</p><ol><li>Frontalization via Homography</li></ol><p><img src="/assets/eco/eco3.png" alt="eco3"></p><ol start="2"><li>Rescaling for canonical depth viewpoint</li></ol><p><img src="/assets/eco/eco4.png" alt="eco4"></p><p>My work mainly focus on the egocentric recognition of sections in supermarket with a novel interface by leveraging scene geometry and reconstructed camera motion.</p><ol><li><p>Undistort the image using camera intrinsic parameters</p><p>Assume that sections are aligned with the three principal orthogonal directions of the scene.</p></li><li><p>Calculate three mutually orthogonal vanishing points. We manually select points $vp_x$ and $vp_y$ in X and Y directions in camera coordinate system.<br>Consider the 3D point where the X axis (in camera coordinates) meets the projective line corresponding to $vp_x$,  let this point be $Xp$. Using K for the camera intrinsic parameters, R and C for pose, we must have</p></li></ol><p>$$\lambda v p_x=KR(X_p-C)\Rightarrow X_p-C=\lambda R^T K^{-1} v p_x$$</p><p>The X axis direction in 3D is thus given by unit $(Xp−C)$. Similarly, the Y direction and the gravity vector is obtained as the cross product of the two. We observe that using the cross product gives more stable gravity vector compared to simply using the output from the algorithm.</p><p><img src="/assets/eco/eco5.png" alt="eco5"></p><ol start="3"><li><p>Triangulate an origin point in 3D using pixel correspondence between two images. </p></li><li><p>Using the origin and axes, construct a bounding box in 3D that is projected onto the image.</p></li><li><p>Keyboard input to the interface can be used to move each of the faces of the box in the normal direction.</p></li></ol><p><img src="/assets/eco/eco6.png" alt="eco6"></p><ol start="6"><li>propagate labels to the box.</li></ol><p><img src="/assets/eco/eco7.png" alt="eco7"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h3&gt;&lt;p&gt;Trying to build a cognitive ma
      
    
    </summary>
    
    
    
      <category term="Computer Vision, 3D reconstruction" scheme="http://yoursite.com/tags/Computer-Vision-3D-reconstruction/"/>
    
  </entry>
  
  <entry>
    <title>Food web visualization</title>
    <link href="http://yoursite.com/2019/11/02/Food-web-visualization/"/>
    <id>http://yoursite.com/2019/11/02/Food-web-visualization/</id>
    <published>2019-11-03T01:14:36.000Z</published>
    <updated>2021-09-25T12:21:02.249Z</updated>
    
    <content type="html"><![CDATA[<p>This is a visualization project with Bell Museum at St. Paul, Minnesota, presented on May 4th, 2019. Intended audience are visitors to the Museum at all age, biology researchers, university students.</p><p>I work in small team with Yichun Yan, instructed by Prof. Daniel Keefe and Bell Museum. I’m mainly responsible for the visual part, including model design, coding in Processing and Javascript (P5.js) for the model and UI design for the website.</p><p>It is a visualization task for understanding the physical reality of food webs. In order to depict food webs graphically, I designed several models that capture the richness, topology as well as dynamics of the system, and served as a comparison to the existing display at the museum. As a visualization tool for the audience at the museum, it proved to have an excellent user experience, which allows for convenient interaction. The audience was guided through an interactive tour from Lake Superior to the ocean, from ecosystem to every single species inside. There is also a game inside, which is creative and fun. Children were especially engaged during our demonstration; they were excited to learn using this tool. What makes the project stands out is that they make connections to the knowledge we covered in class, including color, graphs, and storytelling to convey information. </p><p>Presentation website demo: <a href="https://joeblack220.github.io/FoodWebPage/?nsukey=1JrIJp0lHfImNxq17HF2aN2piQ687xhkzAhXudEHxsIWGVYgI%2FVZMY9ffYoiuPJxrt%2BV0sRyH19%2Bd7iqb5s6VJaUkvYChlpVJOaU5RD8Js1JMz5%2BilPlpbkyNPqM8Xz013jTlraRd5fNj19FokGQVQ%3D%3D" target="_blank" rel="noopener">FoodWebVis</a>, <a href="https://github.com/JoeBlack220/CSci5609" target="_blank" rel="noopener">Code</a></p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>The first page of the website:</p><p><img src="/assets/vis/vis1.gif" alt="vis1"></p><p>shows a randomly generated food chains which corresponds with the the food web which is presented at Bell Museum:</p><p><img src="/assets/vis/vis6.png" alt="vis6"></p><p>An food web example in Minnesota:</p><p><img src="/assets/vis/vis2.png" alt="vis2"></p><p><img src="/assets/vis/vis3.png" alt="vis3"></p><h3 id="Food-web-in-ecosystems"><a href="#Food-web-in-ecosystems" class="headerlink" title="Food web in ecosystems"></a>Food web in ecosystems</h3><p>However in real world, food webs are much more complex than the ideal model in the museum, thus they cannot be represented by the method above. In order to help audience learn more about the real food webs, we design several ways of visualization. </p><p>Click on the Ecosystem button on the menubar, and choose the ecosystem that you are interested in from the dropdown. For example, Carpinteria from <a href="https://www.nceas.ucsb.edu/interactionweb/index.html" target="_blank" rel="noopener">IWDB</a>, which is a salt marsh. The layout is as follows:</p><p><img src="/assets/vis/vis4.png" alt="vis4"></p><ol><li>The 2D circle visualization puts each species around the big circle and connect each pair of interaction(e.g. predator-pray) with lines. Color represents different groups. In this way, we can learn about the complexity directly. Background is a picture of this ecosystem. </li></ol><ul><li>click on the “subwebs” labels on the left bottom to select which subweb(s) to display, in order to simplify the model,</li></ul><p><img src="/assets/vis/vis7.gif" alt="vis7"></p><ul><li>click on each point on the circle to learn about this species, the information is shown on the left box, and the food chain around it is shown below. It helps audience learn what’s in this ecosystem in details,</li></ul><p><img src="/assets/vis/vis8.gif" alt="vis8"></p><ul><li><p>Although richness and topology are captured by graphic depictions, the utility of the depictions is often limited to impressing upon the viewer the overwhelming structural complexity of the systems. In fact, it is more complicated than illustrated, being based on a dataset comprising 128 species and 2290 interspecific interactions. We need to provide some statistics, for example, connectance, a number of measures and summary statistics are used to describe and compare food webs, which shown on the left bottom side. C = L/S^2, where L is the number of directional links in the network, and S is the number of nodes or species. Connectance values are generally well below one, reflecting the relative sparsity of links in food webs.</p><p>Audience can compare the number of species and interaction, connectance between different ecosystems. What’s more, they can double click on the species on the big circle to remove the species from the system, connectance will change. However, different species play different role in the ecosystem, thus having different influence on the ecosystem. It is an innovative approach to discover the importance of a certain species. </p><p><img src="/assets/vis/vis10.gif" alt="vis10"></p><p>Notice that no species can be independent from the whole ecosystem, every one of them is indispensable to the food web.</p></li></ul><ol start="2"><li>The animated 3D structure put species of the same trophic level at certain height around a circle, and shows how energy flow in to nect trophic level by connecting each interaction. It is hierarchyical model which is similar to the Ecological Pyramid (or Trophic pyramid). Unlike the 2D visualization that puts all links inside one circle, which is messy, this model describles the structure of each trophic level clearly, and animates the process of energy flow. It allows for user interaction. Audience can zoom in/out and rotate with mouse control. </li></ol><p><img src="/assets/vis/vis9.gif" alt="vis9"></p><ol start="3"><li>The interaction matrix which is shown on the bottom is a simple and direct way to depict the interactions in the ecosystem, to get an overview of structure and complexity.</li></ol><p><img src="/assets/vis/vis5.png" alt="vis5"></p><p>Besides Carpinteria, there are other ecosytems for audience to explore:</p><ul><li>Baja California, an ocean ecosystem, is much more complicated. In the ocean, small fish is eaten by big fish, big fish is eaten by bigger fish… thus forms long food chains. There are 11 trophic levels:</li></ul><p><img src="/assets/vis/vis13.gif" alt="vis13"></p><ul><li>Berwick, a pine forest:</li></ul><p><img src="/assets/vis/vis14.gif" alt="vis14"></p><h3 id="Energy-pyramid"><a href="#Energy-pyramid" class="headerlink" title="Energy pyramid"></a>Energy pyramid</h3><p>Click on “Energy” on the menu. This is a small game to illustrate energy pyramid and significance of environmental protection. </p><p>An energy pyramid is shown as below, a graphical representation designed to show the biomass or bioproductivity at each trophic level in a given ecosystem. each point stands for a species, the lowest level consistd of producers like plants and alges, top level are mainly big predators.<br>Click on speices to add energy or plastic waste to it, and see how they transfer along the food web:</p><ol><li>An energy pyramid is a presentation of the trophic levels in an ecosystem. Energy from the sun is transferred through the ecosystem by passing through various trophic levels. Roughly 10% of the energy is transferred from one trophic level to the next, thus preventing a large number of trophic levels. There must be higher amounts of biomass at the bottom of the pyramid to support the energy and biomass requirements of the higher trophic levels.</li></ol><p><img src="/assets/vis/vis11.gif" alt="vis11"></p><ol start="2"><li>When plastic is part of the food chain:<br>Microplastics are plastic pieces smaller than 5 mm. This can include extremely small plastic pieces used in household products, clothing fibres and large plastic pieces that degrade into smaller fragments.<br>Microplastics can accumulate in fish, birds and other marine life. Because they do not break down rapidly, the amount in the sea and fish will continue to accumulate, making the problem worse over time, until we manage to reduce the amount of plastic in the sea.</li></ol><p><img src="/assets/vis/vis12.gif" alt="vis12"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This is a visualization project with Bell Museum at St. Paul, Minnesota, presented on May 4th, 2019. Intended audience are visitors to th
      
    
    </summary>
    
    
    
      <category term="Visualization, Food Web, Processing, Web, UI" scheme="http://yoursite.com/tags/Visualization-Food-Web-Processing-Web-UI/"/>
    
  </entry>
  
  <entry>
    <title>Undersea Game</title>
    <link href="http://yoursite.com/2019/10/27/undersea/"/>
    <id>http://yoursite.com/2019/10/27/undersea/</id>
    <published>2019-10-28T01:05:47.000Z</published>
    <updated>2021-09-25T12:16:56.854Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/assets/undersea/undersea2.gif" alt="undersea2"></p><p>Full Demo: <a href="https://www.youtube.com/watch?v=P3lN6DseqMA&t" target="_blank" rel="noopener">Youtube link</a>, <a href="https://github.com/SnowflyLXF/Computer-Graphics-Practices/tree/master/underseaGame" target="_blank" rel="noopener">Code</a></p><h3 id="Project-Description-amp-Overview"><a href="#Project-Description-amp-Overview" class="headerlink" title="Project Description &amp; Overview"></a>Project Description &amp; Overview</h3><p>Creating a 3D game in OpenGL is both challenging and rewarding. There are many resources and libraries available to create a very realistic and high-end video game. I made use of OpenGL and SDL library to create a 3D adventure game that is set undersea. This game requires navigating through a 3D virtual scene to catch jellyfishes meanwhile avoid sharks to reach the goal. By implementing advanced features, the overall game play quality went up as well as the look and feel of the game.<br>My goal is to create a realistic undersea world. The major task is to make the scene realistic. I need to implement complex objects and textures in the scene as well as special effects like shadows, fog and caustic. Another obstacle is to add text to the game. Moreover, a smooth motion of the character by keyboard input and a friendly menu with a mini-map will add to a better experience.</p><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><ol><li>Rendering Pipeline is the sequence of steps that OpenGL takes when rendering objects. Vertex attribute and other data go through a sequence of steps to generate the final image on the screen. First apply transform in 3D world coordinate system, next add illuminate according to lighting and reflectance. Then transform into 3D camera coordinate system, then 2D screen coordinate system, Clip primitives outside camera’s view. Finally, draw Pixels, also texturing.<br>I use glTranslatef and glRotatef for matrix transformation. I also apply the perspective switch by F2 to show the transformation:</li></ol><p><img src="/assets/undersea/undersea3.png" alt="undersea3"></p><ol start="2"><li>Lights I used include ambient light, which a scene-wide global ambient light to be applied to all primitives; directional light that models point light at infinity at certain direction; spotlight as a headlight that falls out smoothly and cut off. The images below show the effect of spotlight, you can turn it off by F3.</li></ol><p><img src="/assets/undersea/undersea4.png" alt="undersea4"></p><ol start="3"><li>Shadow is also very important to detect objects around. It is created by projection from the 3D project to the ground turning the projection to a fixed dark color. The projection is scaled a little to make it bigger when it gets closer to the ground. You can turn it off by F5 if you don’t like it.</li></ol><p><img src="/assets/undersea/undersea5.png" alt="undersea5"></p><ol start="4"><li>For the color of the scene, I use Gouraud shading model that calculates the surface normals at the vertices of polygons in a 3D computer model. These normals are then averaged for all the polygons that meet at each point. Lighting computations are then performed to produce color intensities at vertices.<br>Also the reflections and refractions. One of the special effects of water is caustic, referring to patches of light or their bright edges. It is caused by light rays reflected or refracted by a curved surface or object, or the projection of that envelope of rays on another surface. I create this effect by adding 3 layers of texture to the sea bottom. You can also turn off the effect by F4.</li></ol><p><img src="/assets/undersea/undersea6.png" alt="undersea6"></p><h3 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h3><p>Build on Ubuntu 16.04. Install dependencies: </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install libsdl2-dev</span><br><span class="line">$ sudo apt-get install libsdl2-ttf-dev</span><br><span class="line">$ sudo apt-get install libsdl2-image-dev</span><br></pre></td></tr></table></figure><p>Download the files,  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/SnowflyLXF/OpenGL-3D-game.git</span><br></pre></td></tr></table></figure><p>run the makefile inside the source code folder,</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> OpenGL-3D-game/project/src</span><br><span class="line">$ make</span><br></pre></td></tr></table></figure><p>run the executable file to start the game:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ./Undersea</span><br></pre></td></tr></table></figure><h3 id="Manual-control"><a href="#Manual-control" class="headerlink" title="Manual control"></a>Manual control</h3><ol><li>Startup</li></ol><p>Once you start the game, a tutorial appears for 10s then disappears(see tutorial.cpp). As you can see from the image, there is a stopwatch showing the remaining time (starting at 90s), a scoreboard showing accumulative scores up till now, and a mini-map at the left bottom corner, showing the position of sharks (marked by red spot) and jellyfishes (marked by blue spot)(see minimap.cpp). I use SDL-TTF to render text, the font is Oswald-bold(see textrender.cpp).</p><p><img src="/assets/undersea/undersea1.png" alt="undersea1"></p><ol start="2"><li>Control</li></ol><p>Direction Keys:</p><p>Key ‘w’: Move forward, Key ‘s’: Move backward</p><p>Key ‘q’: Move upward, Key ‘e’: Move downward</p><p>Key ‘a’: Turn left, Key ‘d’: Turn right</p><p>Other:</p><p>Key ‘ESC’: Terminate the game</p><p>key SPACE: Accelerate</p><p>F1: Help  F2: switch perspective F3: Headlight  F4: Reflection  F5: Shadow</p><p>In order to move more smoothly and realistically, I apply Newton’s law of Motion to the object. F=ma, taking friction in water into account F=-f (f for friction, which is set to constant, see Patrick.h). Speed will decrease to zero if no extra force is imposed, Δv = a. If Space is pressed, you will obtain acceleration in the moving direction. “Turbo” will appears on the top right corner. Notice that you can only use this function once in 5s. The velocity and acceleration are calculated as vector and are updated with the control system(see Dostep function in Patrick.cpp).</p><ol start="3"><li>Collision Detection</li></ol><p>Since there are various objects located in the game, the implementation of collision detection is required to detect when the player the items. Each object is bounded by a rectangular box–bounding box(in Mesh.cpp ComputeBoundingBox). A collision is detected when the volumes of the two objects’ boxes intercept one another. The condition for a collision is: for each dimension (i.e. x,y,z), there is an overlap between the space occupied by object 1 and object 2 in that dimension. Once you collide with a jellyfish, you will gain 10 points, Once you collide with a shark, game over.</p><h3 id="Game-Design"><a href="#Game-Design" class="headerlink" title="Game Design"></a>Game Design</h3><p>In creating the game, many different OpenGL programming techniques are used for drawing and viewing objects and adding special effects.</p><ol><li>Characters</li></ol><p>Items in the game are drawn with different shapes which are loaded from obj files(). They comprise of triangle meshes. The mesh components are vertices, edges and triangles. First read all the vertices, normals, texcoords &amp; triangles into the allocated arrays. Then use Gouraud shading model that computes the lighting at the corners of each triangle and linearly interpolating the resulting colors for each pixel covered by the triangle. (see mesh.cpp)</p><p><img src="/assets/undersea/undersea7.png" alt="undersea7"></p><p>Patrick is the major character of the game that represents the player. The game uses a first-person view, so you can only see the back of him.<br>Jellyfish is the bonus(see Jellyfish.cpp). They are generated randomly. A new jellyfish is added every 3s in random position at the same height, and falling at a certain speed, it disappears beneath the ground(see JellyManager.cpp). Notice that jellyfish is translucent, to create the effect, I use blend function of OpenGL to decrease the alpha value of the object to allow some light to travel through jellyfishes.</p><p><img src="/assets/undersea/undersea8.png" alt="undersea8"></p><p>Shark is the villian, there are 3 sharks in total moving towards you at a certain speed(see Enemy.cpp, EnemyManager.cpp). Other objects like seaweed and starfish at sea bottom are for decoration (see Map.cpp).<br>Transformation functions including glRotatef and glTranslatef are applied to each object.</p><ol start="2"><li>Camera</li></ol><p>The camera in the game is defined with the gluLookAt function. Throughout the main part of the game, the player explores the maze in the first-person view. He is able to look around the maze from his current position by using keyboard. This was done with SDL_Keycode which allows the game to record keyboard input.</p><ol start="3"><li>Lighting and Material Variation</li></ol><p>Light sources, namely, specular, diffuse and ambient, are set. The light sources settings are initialized with the function glLightfv. These light sources are created with the code below(a direction light).<br>The diffuse light is used to define the bright areas of the object and the ambient light defines the color of the object.</p><p>Another effect is fog by glFog.</p><ol start="4"><li>Texture Mapping</li></ol><p>In designing the game, we used the technique of texture mapping in order to provide a professional touch to our graphics. Texture mapping is the process of coloring pixels based on an image. In our game, we used bitmaps as our textures and mapped them to different objects in the game.</p><p><img src="/assets/undersea/undersea9.png" alt="undersea9"></p><h3 id="Future-Work"><a href="#Future-Work" class="headerlink" title="Future Work"></a>Future Work</h3><ol><li><p>Add more characters like different fish as enemy to decrease your score or bonus to increase score. For example, collision with octopus-lose 10 points; collision with barracuda-gain 5 points.</p></li><li><p>Procedurally generate levels</p></li><li><p>Add more functions like attacking the enemy</p></li><li><p>Simulate the water effect of flow as well as seaweed and jellyfish floating with the flow to make it more realistic</p></li><li><p>create an intelligent agent that plays the game automatically</p></li></ol><h3 id=""><a href="#" class="headerlink" title=""></a></h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/assets/undersea/undersea2.gif&quot; alt=&quot;undersea2&quot;&gt;&lt;/p&gt;
&lt;p&gt;Full Demo: &lt;a href=&quot;https://www.youtube.com/watch?v=P3lN6DseqMA&amp;t&quot; targ
      
    
    </summary>
    
    
    
      <category term="C++, OpenGL, 3D, game" scheme="http://yoursite.com/tags/C-OpenGL-3D-game/"/>
    
  </entry>
  
  <entry>
    <title>Dempost</title>
    <link href="http://yoursite.com/2019/10/09/Dempost/"/>
    <id>http://yoursite.com/2019/10/09/Dempost/</id>
    <published>2019-10-09T06:19:22.000Z</published>
    <updated>2019-11-11T09:46:11.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/assets/dempost/dem.gif" alt="dem"></p><h3 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h3><p>This  is my undergradute thesis at Fudan University, when I was pursuing a BS in ME. It mainly studies the mechanical model of particles based on discrete element method and develops a complete post-processing software: DEMPost. I used OpenGL to visualize the animation of Particle Flow. This is my first step into Computer Graphics, I was overwhelmed by its charm. </p><p>Particles are common form of matter in nature and are also ubiquitous in production and life; Discrete Element Method(DEM) is a numerical method used to calculate how large particles move under given conditions. This passage only talks about the software part of the thesis, to see the full version in Chinese <a href="https://drive.google.com/file/d/1w5BFQWmnvTEH_t2XbZgwuxoL_Jj9EsHe/view?usp=sharing" target="_blank" rel="noopener">Thesis</a>.</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>The design and development of discrete element post-processing software DEMPost is the most important part of my work. Post-processing program is an important auxiliary of discrete-element numerical simulation program. This paper develops DEMPost with ApenPost as an example. In addition to satisfying the basic requirements of post-processing program to visualize and post-process discrete element models and related variables, I also added loading of discrete-element calculation data, animated display over time, and histograms showing the distribution and changes of main variables (mass, velocity, acceleration, etc). It also have strong interactivity. The design, development and maintenance of the software are all done independently by Xuefei Li from January to May 2018.</p><p>The discrete element post-processor needs to be developed and used with a graphics supported system with strong interactivity and sufficient memory space for operations. This software was developed on MacBook Pro (15-inch, 2016). The system configuration is 2.6 GHz Intel Core i7, 16 GB 2133 MHz LPDDR3, Intel HD Graphics 530 1536 MB. DEMPost post-processor chooses Python, which is an advanced object-oriented, dynamic data type advanced computer programming language that follows the GPL (GNU General Public License) protocol. </p><p>The main function of this software is to visualize the discrete element numerical simulation results, I choose OpenGL (Open Graphics Library) as the application interface. OpenGL is a professional graphical program interface for cross-platform and programming languages. It is powerful, rich and easy to use. It is mainly used to process 3D images and is also suitable for 2D images. In CAD, Virtual Reality, scientific visualization programs and video game development, etc. wxPython is an excellent cross-platform GUI library for Python, adding a number of extensions to wxWidgets that encapsulate other famous GUI graphics library for Python. With wxPython, you can design a robust and practical GUI; this article also uses other libraries, such as numpy, matplotlib, etc., which will be briefly explained later. In this chapter, I will introduce the DEMPost program structure and the interfaces used.</p><p>DEMPost includes, but are not limited to, the following functions: reading and loading data of the particle stream, including particle states (position, velocity, acceleration, etc.) at different times and rendering; animating the particle stream according to time series; animating particle flow of independent physical quantities according to time series; reading, loading and drawing force chain data (position, size, etc.) ; the force chain is animated according to the time series; performing statistics and drawing histogram of the physical quantities of force chain (velocity, Acceleration, size, etc.); drawing and animation of velocity acceleration fields at different moments; strong interactive functions, which allow the users to rotate, pan, zoom in/out, etc. directly through the mouse and keyboard.</p><p>Since the passage is too long, you can simply take a look at the Manual. However, the remaining parts talk about the implementation of the software, you can skip them.</p><h3 id="Manual"><a href="#Manual" class="headerlink" title="Manual"></a>Manual</h3><ol><li>Import and visualization of particle data</li></ol><p>“File-&gt;Import” (Ctrl+I), defined in the form class canvas-Frame, using wxPython’s directory dialog wx.DirDialog to pass the selected file/folder location as a parameter to In the DEMPost class,</p><p><img src="/assets/dempost/dem3.png" alt="dem3"></p><p>The data is calculated with DEM, which is stored inr the folder ForceeChain20170426, and the state data of the particles is in the subfolder /file. Each dat file in the catalog represents the flow of particles at a certain time, where each row represents a particle, a total of 21 columns. Information of data structure is explained in ReadMe.dat. In order to improve efficiency, load_data.py performs data cleaning, leaving only the following information of position, velocity and acceleration: (x_i, y_i, z_i, v{xi}, v{yi}, v{zi}, a{xi}, a{yi}, a{zi}) i=1, 2, 3,…. The list is passed to DEMPost.</p><ol start="2"><li>Parameter settings</li></ol><p>After the file is determined, determine the object type of the model, select it in “File-&gt;Object” (Ctrl+O), use wxPython’s select list dialog box, “Particle Flow”, “Force Chain”, “Velocity”, “Acceleration” to select the four objects, and select the result that passed to DEMPost,</p><p><img src="/assets/dempost/dem4.png" alt="dem4"></p><p>After the file is determined, determine the object type of the model, select it in “File-&gt;Object” (Ctrl+O), use wxPython’s select list dialog box, “Particle Flow”, “Force Chain”, “Velocity”, “Acceleration” to select the four objects, and select the result that passed to DEMPost.</p><p>To draw a particle flow model, you only need to use the coordinates, and pass the collection of individual particle coordinates to the combination class Particles.<br>Note that ParticleFlow also needs another parameter magnify, which represents the scaling factor of the particle coordinates, which is changed by “Edit-&gt;Parameter”(Ctrl+E). The process is as follows, click on the “Parameter” trigger text input dialog box, and the obtained user input is passed to the DEMPost canvas layer.</p><ol start="3"><li>Particle Flow visualization</li></ol><p>When the parameters are set, the particles can be drawn. In the menu bar, “Display-&gt;Show” (Ctrl+R), the implementation process is the same as the binding event mentioned in the previous two sections. The processed data is imported to the Particle stream datalist on DEMPost canvas to update canvas. Notice that there are no boundaries in the diagram, click “File-&gt;Import” Border to execute:</p><p><img src="/assets/dempost/dem5.png" alt="dem5"></p><p>Use toolbar to control animation(Start, stop, next frame, previous frame).  “Display-&gt;Clear(Ctrl+W)” clear the canvas and set the datalist, vellist, alist, linklist in the DEMPost canvas to an empty list to refresh the canvas.</p><p><img src="/assets/dempost/dem6.png" alt="dem6"></p><ol start="4"><li>Velovity visualization</li></ol><p>“File-&gt;Object”(Ctrl+O): select “Velocity” in the pop-up dialog box, the result is passed to the DEMPost canvas, and the datatype becomes”‘Velocity”.</p><p>Since velocity is a vector, creating a composite class Vector in node.py. The particle velocity vector consists of particles and velocity vectors. It is an arrow shape consists of a Sphere and a Vector. The sphere is in the center of the corresponding particle, representing the position of the particle, Vector represents the velocity vector, the arrow points to the direction of the velocity vector, and the mode and velocity vector in proportion to the model,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Vector</span><span class="params">(HierarchicalNode)</span>:</span> <span class="comment"># the mode of the vector is 1, r is the radius</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, l, r)</span>:</span></span><br><span class="line">super(Vector, self).__init__() </span><br><span class="line">        self.child_nodes = [Cylinder(), Cone()] </span><br><span class="line">        self.child_nodes[<span class="number">0</span>].color_index = <span class="number">1</span> </span><br><span class="line">        self.child_nodes[<span class="number">0</span>].scale(r, r, l) </span><br><span class="line">        self.child_nodes[<span class="number">1</span>].color_index = <span class="number">2</span> </span><br><span class="line">        self.child_nodes[<span class="number">1</span>].translate(<span class="number">0</span>, <span class="number">0</span>, l) </span><br><span class="line">        self.child_nodes[<span class="number">1</span>].scale(r, r, r)</span><br><span class="line">   </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ManyVectors</span><span class="params">(HierarchicalNode)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, p, di, mag)</span>:</span></span><br><span class="line">        super(ManyVectors, self).__init__() </span><br><span class="line">        self.child_nodes = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(p)):</span><br><span class="line">        self.child_nodes.append(Sphere()) </span><br><span class="line">            self.child_nodes[index].scale(<span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.3</span>) </span><br><span class="line">            self.child_nodes[index].translate(p[i][<span class="number">0</span>]*mag, p[i][<span class="number">1</span>]*mag,p[i][<span class="number">2</span>]*mag)</span><br><span class="line">        self.child_nodes[index].color_index = <span class="number">7</span></span><br><span class="line">        cl = sqrt(di[i][<span class="number">0</span>]*di[i][<span class="number">0</span>]+di[i][<span class="number">1</span>]*di[i][<span class="number">1</span>]+di[i][<span class="number">2</span>]*di[i][<span class="number">2</span>])*mag</span><br><span class="line">        self.child_nodes.append(Vector(cl, <span class="number">0.3</span>)) self.child_nodes[index].rotate(di[i][<span class="number">0</span>], di[i][<span class="number">2</span>], di[i][<span class="number">1</span>]) </span><br><span class="line">            self.child_nodes[index].translate(p[i][<span class="number">0</span>]*mag, p[i][<span class="number">1</span>]*mag, p[i][<span class="number">2</span>]*mag)</span><br></pre></td></tr></table></figure><p>Like the particle stream, the value passed in DEMPost also represents the number of frames in the picture. It can be animated and controlled by the four buttons of the toolbar. It can be seen from the animation that when the particles are deposited at the bottom, the velocity is small. When some particles move above the boundary, the overall velocity becomes larger, the bottom velocity is upward, and the velocity is reversed after collision with the upper edge, and the particles are scattered around during the falling process. The velocity vector is radial. The velocity cloud map is used to represent the velocity,</p><p><img src="/assets/dempost/dem7.png" alt="dem7"></p><p><img src="/assets/dempost/dem8.png" alt="dem8"></p><ol start="5"><li>Acceleration visualization</li></ol><p>Acceleration vector diagram is similar to velocity. In order to distinguish, the author changed the color and adjusted to the appropriate parameter.</p><p><img src="/assets/dempost/dem9.png" alt="dem9"></p><p><img src="/assets/dempost/dem10.png" alt="dem10"></p><ol start="6"><li>Velocity and acceleration statistics</li></ol><p>n order to observe the distribution of particle flow velocity and acceleration, DEMPost provides the function of displaying the histogram of variable distribution. Click “Display-&gt;Plot”</p><p><img src="/assets/dempost/dem11.png" alt="dem11"></p><p><img src="/assets/dempost/dem12.png" alt="dem12"></p><p>The object of statistics is the modulus of all velocities and accelerations, using the wxPython backend of matplotlib, creating the FigureCnavas object, embedding matplotlib into the GUI.</p><p><img src="/assets/dempost/dem13.png" alt="dem13"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib.backends.backend_wxagg <span class="keyword">import</span> FigureCanvasWxAgg <span class="keyword">as</span> FigureCanvas</span><br><span class="line"><span class="keyword">from</span> matplotlib.backends.backend_wx <span class="keyword">import</span> NavigationToolbar2Wx <span class="keyword">from</span> matplotlib.figure </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CanvasPanel</span><span class="params">(wx.Panel)</span>:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, parent, plot_data1, plot_data2)</span>:</span></span><br><span class="line">wx.Panel.__init__(self, parent)</span><br><span class="line"><span class="comment"># velocity</span></span><br><span class="line">self.figure = Figure(figsize=(<span class="number">5</span>,<span class="number">3</span>))</span><br><span class="line">self.axes = self.figure.add_subplot(<span class="number">211</span>) </span><br><span class="line">        self.axes.hist(plot_data1, <span class="number">40</span>, normed=<span class="literal">True</span>) </span><br><span class="line">        self.axes.set_title(<span class="string">'Velocity'</span>) </span><br><span class="line">        self.axes.set_ylabel(<span class="string">'count'</span>)</span><br><span class="line"><span class="comment"># acceleration</span></span><br><span class="line">self.axes = self.figure.add_subplot(<span class="number">212</span>) </span><br><span class="line">        self.axes.hist(plot_data2, <span class="number">40</span>, normed=<span class="literal">True</span>) </span><br><span class="line">        self.axes.set_title(<span class="string">'Acceleration'</span>) </span><br><span class="line">        self.axes.set_ylabel(<span class="string">'count'</span>)</span><br><span class="line">self.canvas = FigureCanvas(self, <span class="number">-1</span>, self.figure) </span><br><span class="line">        self.sizer = wx.BoxSizer(wx.VERTICAL) </span><br><span class="line">        self.sizer.Add(self.canvas, <span class="number">1</span>, wx.LEFT | wx.TOP | wx.GROW) </span><br><span class="line">        elf.SetSizer(self.sizer)</span><br><span class="line">self.Fit()</span><br></pre></td></tr></table></figure><ol start="7"><li>Force chain visualization</li></ol><p>Force chain grids are often used in particle mechanics methods to represent the amount of force that exists between each pair of particles. DEMPost can also load and visualize a 3D force chain network. The force chain data/chains are also imported via File-&gt;Import(Ctrl+I). The data structure in each file is as follows,</p><p><img src="/assets/dempost/dem14.png" alt="dem14"></p><p>Each row represents a force chain, the ith rows 1, 2, and 3 represent the xyz coordinates of the first and first particles in the i-th force chain in the force chain, and the fourth, fifth, and sixth columns represent the xyz coordinates of the second particle. Column 7 represents the magnitude of the force. In order to simplify the model, the force chain is regarded as a ManyVector arrow. The starting point is the first particle and the ending point is the second particle. The thickness of the wire is proportional to the force.</p><p><img src="/assets/dempost/dem15.png" alt="dem15"></p><p><img src="/assets/dempost/dem16.png" alt="dem16"></p><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><ol><li>3D Modeling and OpenGL</li></ol><p>Computer-Aided Design (CAD) models the 3D world and creates and designs it on a computer. The key step in 3D modeling is rendering, which is to store and render as many and complex objects as possible, while keeping the rendering program less complex. Before rendering, you first need to create a window, because the graphics-driven manipulation is not enough, so you need to use a cross-platform image application interface OpenGL and OpenGL tool library GLUT to manage the window.</p><p>The render object of OpenGL is a polygon composed of vertices and normal vectors. Currently OpenGL has two branches: traditional OpenGL and modern OpengL. Traditional OpenGL provides a fixed pipeline. By adjusting global variables, developers can enable or disable some rendering features such as lighting, shading, and more. Next, OpenGL automatically renders the scene based on the features selected; modern OpenGL uses a programmable pipeline instead of a fixed pipeline, and developers only need to write shaders applets on graphics hardware such as GPUs. Traditional methods are relatively simple, and this article still uses traditional OpenGL programming. GLUT is a library bundled with OpenGL that creates an action window callbacks for the user interface. Due to the high requirements for frame management and user interface in this article, a complete library is required, thus wxPython interface is also used later.</p><p>In computer graphics, the basic elements are coordinate systems, points, vectors, and transformation matrices. The coordinate system consists of the origin and three basic vectors, denoted as the x-axis, y-axis, and z-axis; points in 3D coordinates can be represented as offsets to the origin in the x, y, and z directions. The vector represents the difference between the two points in the x-axis, y-axis, and z-axis directions; the transformation matrix can realize the conversion between coordinate systems. To convert a vector ν in one coordinate system to another, you can multiply a transformation matrix M:</p><p>v` = Mv</p><p>Common transformations include panning, zooming in/out, rotating, etc., which are called affine transformations. Figure below shows some of the transformations required from the physical object to the screen. The right half of the figure is the transformation of the object coordinate system (eye space) to the screen view (Viewort Space) can be implemented with OpenGL’s own functions; the inverse transformation from the object coordinate system to the screen view can be handled by gluPerspective, from Normalized device space, the conversion to the screen view can be processed by glViewport, and the two matrices are multiplied and stored as a GL PROJECTION matrix. The left half of the figure is implemented by the developer, defining a transformation matrix from the point in the model to the entity object as the model matrix, and then defining a transformation matrix from the entity object to the eye as the view matrix (view matrix ) view matrix, these two matrices are combined to get the ModelView matrix. The transformation of the 3D model The matrix is a four-dimensional matrix, because in the translation transformation, the fourth element determines whether the tuple is a point or a vector in space.</p><ol start="2"><li>Fundamental architecture of OpenGL</li></ol><p>First, configure the lab environment and install python-opengl</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt−get install python−opengl</span><br></pre></td></tr></table></figure><p>Once installed, you can start building the viewer class DEMPost, which is implemented in viewer.py.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DEMPost</span><span class="params">(MyCanvasBase)</span>:</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">InitGL</span><span class="params">(self)</span>:</span></span><br><span class="line">       <span class="string">""" Initialize the viewer. """</span> self.scene = Scene()</span><br><span class="line">       <span class="keyword">if</span> self . size <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">       self.size = self.GetClientSize()</span><br><span class="line">       self .w, self .h = self . size</span><br><span class="line">       <span class="comment"># position viewer</span></span><br><span class="line">       glMatrixMode (GL_MODELVIEW) glTranslatef (<span class="number">0.0</span> , <span class="number">0.0</span> , −<span class="number">2.0</span>)</span><br><span class="line">       <span class="comment"># position object</span></span><br><span class="line">       glRotatef(self.y, <span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>) glRotatef(self.x, <span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>)</span><br><span class="line">               print(<span class="string">"\033[4;30;102m INITIALIZE GL \033[0m"</span>)</span><br><span class="line">               glutInitDisplayMode(GLUT_SINGLE | GLUT_RGB)</span><br><span class="line">           glEnable(GL_CULL_FACE)</span><br><span class="line">               glCullFace(GL_BACK)</span><br><span class="line">               glEnable(GL_DEPTH_TEST)</span><br><span class="line">               glDepthFunc(GL_LESS)</span><br><span class="line">               glEnable(GL_LIGHT0)</span><br><span class="line">               glLightfv(GL_LIGHT0, GL_POSITION, GLfloat_4(<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>)) </span><br><span class="line">               glLightfv(GL_LIGHT0, GL_SPOT_DIRECTION, GLfloat_3(<span class="number">0</span>, <span class="number">0</span>, <span class="number">-1</span>)) </span><br><span class="line">               glColorMaterial(GL_FRONT_AND_BACK, GL_AMBIENT_AND_DIFFUSE) </span><br><span class="line">               glEnable(GL_COLOR_MATERIAL)</span><br><span class="line">               glClearColor(<span class="number">0.4</span>, <span class="number">0.4</span>, <span class="number">0.4</span>, <span class="number">0.0</span>)</span><br></pre></td></tr></table></figure><p>Then rendering functions, update the buffer area,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">OnDraw</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">""" The render pass for the scene """</span> self.init_view()</span><br><span class="line">    glEnable (GL_LIGHTING)</span><br><span class="line">    glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)</span><br><span class="line">        <span class="comment"># Load the modelview matrix from the current state of the trackball</span></span><br><span class="line">        glMatrixMode(GL_MODELVIEW)</span><br><span class="line">        glPushMatrix()</span><br><span class="line">        glLoadIdentity()</span><br><span class="line">        <span class="comment"># store the inverse of the current modelview.</span></span><br><span class="line">        currentModelView = numpy.array(glGetFloatv(GL_MODELVIEW_MATRIX)) </span><br><span class="line">        self.modelView = numpy.transpose(currentModelView) </span><br><span class="line">        self.inverseModelView = inv(numpy.transpose(currentModelView))</span><br><span class="line">        <span class="comment"># render the scene. This will call the render function for each object in the scene</span></span><br><span class="line">        self.scene.render()</span><br><span class="line">        <span class="comment"># draw the grid</span></span><br><span class="line">        glCallList(G_OBJ_PLANE)</span><br><span class="line">        <span class="comment"># flush the buffers so that the scene can be drawn glPopMatrix()</span></span><br><span class="line">        self .SwapBuffers()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_view</span><span class="params">(self)</span>:</span></span><br><span class="line">      <span class="string">""" initialize the projection matrix """</span></span><br><span class="line">      xSize, ySize = glutGet(GLUT_WINDOW_WIDTH), glutGet(GLUT_WINDOW_HEIGHT)</span><br><span class="line">      aspect_ratio = float(xSize) / float(ySize)</span><br><span class="line">  <span class="comment"># load the projection matrix. Always the same</span></span><br><span class="line">      glMatrixMode(GL_PROJECTION)</span><br><span class="line">      glLoadIdentity()</span><br><span class="line">  glViewport(<span class="number">0</span>, <span class="number">0</span>, xSize, ySize)</span><br><span class="line">      gluPerspective(<span class="number">70</span>, aspect_ratio, <span class="number">0.1</span>, <span class="number">1000.0</span>) glTranslated(<span class="number">0</span>, <span class="number">0</span>, <span class="number">-15</span>)</span><br></pre></td></tr></table></figure><p>Scene is the picture presented on the screen, the scene class is created in scene.py, the initialization code is as follows,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Scene</span><span class="params">(object)</span>:</span></span><br><span class="line">    obj_list = []</span><br><span class="line">    <span class="comment"># the default depth from the camera to place an object at</span></span><br><span class="line">    PLACE_DEPTH = <span class="number">15.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment"># The scene keeps a list of nodes that are displayed </span></span><br><span class="line">        self.node_list = list()</span><br><span class="line">        <span class="comment"># Keep track of the currently selected node.</span></span><br><span class="line">        <span class="comment"># Actions may depend on whether or not something is selected </span></span><br><span class="line">        self.selected_node = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">replace_node</span><span class="params">(self, nodes)</span>:</span></span><br><span class="line">        <span class="string">""" update nodes to the scene """</span> </span><br><span class="line">        self.obj_list = [] </span><br><span class="line">        self.obj_list.append(nodes)</span><br><span class="line">        self .render()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">render</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">""" Render the scene. """</span> </span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> self.node_list:</span><br><span class="line">            node.render()</span><br></pre></td></tr></table></figure><p>The objects rendered in the scene are all nodes in the scene. Next, define the node class and implement it in node.py.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">""" Base class for scene elements """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.color_index = random.randint(color.MIN_COLOR, color.MAX_COLOR)</span><br><span class="line">        self.aabb = AABB([<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>], [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>]) </span><br><span class="line">        self.translation_matrix = numpy.identity(<span class="number">4</span>) </span><br><span class="line">        self.scaling_matrix = numpy.identity(<span class="number">4</span>) </span><br><span class="line">        self.rotating_matrix = numpy.identity(<span class="number">4</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">render</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">""" renders the item to the screen """</span></span><br><span class="line">        glPushMatrix() </span><br><span class="line">        glMultMatrixf(numpy.transpose(self.translation_matrix)) </span><br><span class="line">        glMultMatrixf(numpy.dot(self.scaling_matrix,self.rotating_matrix)) </span><br><span class="line">        cur_color = color.COLORS[self.color_index] </span><br><span class="line">        glColor3f(cur_color[<span class="number">0</span>], cur_color[<span class="number">1</span>], cur_color[<span class="number">2</span>]) </span><br><span class="line">        self.render_self()</span><br><span class="line">        glPopMatrix()</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">render_self</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">raise</span> NotImplementedError(</span><br><span class="line">            <span class="string">"The Abstract Node Class doesn't define 'render_self'"</span>)</span><br></pre></td></tr></table></figure><p>Next let’s define specific sphere class Sphere,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Primitive</span><span class="params">(Node)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">      super(Primitive, self).__init__() </span><br><span class="line">        self.call_list = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">render_self</span><span class="params">(self)</span>:</span> </span><br><span class="line">      glCallList(self.call_list)</span><br><span class="line">  </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sphere</span><span class="params">(Primitive)</span>:</span></span><br><span class="line">    <span class="string">""" Sphere primitive """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">super(Sphere, self).__init__() </span><br><span class="line">        self.call_list = G_OBJ_SPHERE</span><br></pre></td></tr></table></figure><p>In addition to the spherical class, this paper also uses the cylindrical Cylinder and the conical Cone, which are all redefined in Node.py. Primitive is a class between a sphere class and a node class, and an important part of the model. Implementing pirmitive in primitive.py is called the glCallList method.All specific objects such as spheres, cubes, etc. are primitives, which can be rendered by OpenGL, glNewList(CALL LIST NUMBER, GL COMPILE) and glEndList() to indicate the beginning and end of a piece of code, and associate this code with one number. Therefore you can simply use glCallList (the associated number) to call this code.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">G_OBJ_SPHERE = <span class="number">2</span></span><br><span class="line">G_OBJ_CYLINDER = <span class="number">4</span></span><br><span class="line">G_OBJ_CONE = <span class="number">5</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_sphere</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""create rendering function list for sphere object"""</span></span><br><span class="line">    glNewList(G_OBJ_SPHERE, GL_COMPILE)</span><br><span class="line">    quad = gluNewQuadric()</span><br><span class="line">    gluDeleteQuadric(quad)</span><br><span class="line">    glEndList()</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_cylinder</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""create rendering function list for cylinder object"""</span></span><br><span class="line">    glNewList(G_OBJ_CYLINDER, GL_COMPILE) </span><br><span class="line">    quad = gluNewQuadric() </span><br><span class="line">    gluCylinder(quad, <span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">1</span>, <span class="number">30</span>, <span class="number">30</span>) </span><br><span class="line">    gluDeleteQuadric(quad)</span><br><span class="line">    glEndList()</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_cone</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""create rendering function list for cone object"""</span> </span><br><span class="line">    glNewList(G_OBJ_CONE, GL_COMPILE)</span><br><span class="line">    quad = gluNewQuadric()</span><br><span class="line">    gluCylinder(quad, <span class="number">0.6</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">50</span>, <span class="number">50</span>) </span><br><span class="line">    gluDeleteQuadric(quad)</span><br><span class="line">    glEndList()</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_primitives</span><span class="params">()</span>:</span></span><br><span class="line">    make_sphere()</span><br><span class="line">    make_cylinder()</span><br><span class="line">    make_cone()</span><br></pre></td></tr></table></figure><p>Where glCallList(G_OBJ_SPHERE) produces a sphere, it can also be combined into a model structure of other complex models, that is the composite node class HierarchicalNode, which is a subclass of Node.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HierarchicalNode</span><span class="params">(Node)</span>:</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">       super(HierarchicalNode, self).__init__() self.child_nodes = []</span><br><span class="line">   </span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">render_self</span><span class="params">(self)</span>:</span></span><br><span class="line">       <span class="keyword">for</span> child <span class="keyword">in</span> self.child_nodes:</span><br><span class="line">       child.render()</span><br></pre></td></tr></table></figure><p>Particle flow is also a combination class, which is a combination of many spherical classes.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ParticleFlow</span><span class="params">(HierarchicalNode)</span>:</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, nodes, magnify)</span>:</span></span><br><span class="line">       super(ParticleFlow, self).__init__() self.child_nodes = []</span><br><span class="line">       <span class="keyword">for</span> index <span class="keyword">in</span> range(len(nodes)):</span><br><span class="line">           self.child_nodes.append(Sphere())</span><br><span class="line">           self.child_nodes[index].scale(<span class="number">0.005</span>*magnify, <span class="number">0.005</span>*magnify, <span class="number">0.005</span>*magnify)</span><br><span class="line">           self.child_nodes[index].translate(nodes[index][<span class="number">0</span>]*magnify, nodes[index][<span class="number">1</span>]*magnify, nodes[index][<span class="number">2</span>]*magnify)</span><br><span class="line">           self.child_nodes[index].color_index = <span class="number">5</span></span><br></pre></td></tr></table></figure><p>The three basic operations of translating, zooming and rotating is implemented by a change matrix in transformation.py.</p><p>In order to realize the user interaction with the scene, for example, to change the angle of view by dragging the mouse, to observe from different angles. Since the camera is fixed, we can only change the angle by moving the scene. Here I use the trackball algorithm to achieve. The principle of the trackball algorithm is that the origin of the world coordinate system is the center of the sphere. The scene is regarded as a ball with a sufficiently large radius, and the line of sight is unchanged. The angle of observation is changed by rotating the ball. In this article, to be brief, I use trackball defined in Glumpy directly. Use wget to download the trackball.py and store it in the working directory. The drag-to function updates the rotation matrix by taking the initial position of the mouse and the position of the mouse after the move, and stores it in the trackball.matrix of DEMPost to implement the scene. In order to rotate and update the ModelView.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">matrix.self.trackball.drag_to(self.mouse_loc[<span class="number">0</span>], self.mouse_loc[<span class="number">1</span>], dx, dy)</span><br></pre></td></tr></table></figure><ol start="3"><li>wxPython interface</li></ol><p>wxPython is a cross-platform GUI toolkit for the Python language. It is an extension of Python, and also an open source software that supports Windows, MacOS and most Unix systems and can be installed directly using pip.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ pip install wxpython</span><br></pre></td></tr></table></figure><p>The GUI consists of a menu bar, a status bar, a toolbar, and a canvas. The first thing is to import wx Library, then create an application object and start the program.</p><p><img src="/assets/dempost/dem1.png" alt="dem1"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> wx</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RunDemoApp</span><span class="params">(wx.App)</span>:</span> </span><br><span class="line">  value = <span class="number">0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span> </span><br><span class="line">      wx.App.__init__(self, redirect=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="string">"""GUI initialization"""</span></span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">OnInit</span><span class="params">(self)</span>:</span></span><br><span class="line"><span class="comment"># create canvas object: DemPost</span></span><br><span class="line">canvasClass = eval(<span class="string">'DEMPost'</span>)</span><br><span class="line"><span class="comment"># create a window</span></span><br><span class="line">self.frame = canvasFrame(<span class="literal">None</span>, <span class="number">-1</span>, <span class="string">'DEMPost'</span>, size=(<span class="number">1000</span>,<span class="number">776</span>),pos=(<span class="number">0</span>,<span class="number">100</span>))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">OnCloseFrame</span><span class="params">(self, evt)</span>:</span> <span class="comment"># exit frame</span></span><br><span class="line"><span class="keyword">if</span> hasattr(self, <span class="string">"window"</span>) <span class="keyword">and</span> hasattr(self.window,<span class="string">"ShutdownDemo"</span>): </span><br><span class="line">          self.window.ShutdownDemo()</span><br><span class="line">evt.Skip()</span><br><span class="line"><span class="comment"># creage an application program</span></span><br><span class="line">app = RunDemoApp() </span><br><span class="line"><span class="comment"># run the program </span></span><br><span class="line">app.MainLoop()</span><br></pre></td></tr></table></figure><p>I create a window canvasFrame with the size of 1000x776 and an initial position of (0, 100), which is the center of the left side of the screen. The menu bar is created in canvasFrame. The menu bar includes four drop-down menus, including File, Edit, Display and Plot. For each area of the management interface, the control panel is also required for initialization.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">self.panel = wx.Panel(self.frame, <span class="number">-1</span>) </span><br><span class="line">self.sizer = wx.BoxSizer(wx.HORIZONTAL)</span><br><span class="line"></span><br><span class="line"><span class="comment"># first canvas</span></span><br><span class="line">self.canvas1 = canvasClass(self.panel) <span class="comment"># passed to MyCanvasBase </span></span><br><span class="line">self.sizer.Add(self.canvas1, <span class="number">1</span>, wx.LEFT | wx.TOP | wx.GROW)</span><br></pre></td></tr></table></figure><p>Subsequent canvases are based on dashboards, such as visual area layers,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">canvasFrame</span><span class="params">(wx.Frame)</span>:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,*args,**kw)</span>:</span></span><br><span class="line">super(canvasFrame,self).__init__(*args,**kw) </span><br><span class="line">        self.InitUI()</span><br><span class="line">plot_data = [<span class="string">'Velocity'</span>, <span class="string">'Acceleration'</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">InitUI</span><span class="params">(self)</span>:</span></span><br><span class="line">menuBar = wx.MenuBar()</span><br><span class="line">filemenu1 = wx.Menu()</span><br><span class="line">fitem = filemenu1.Append(wx.ID_EXIT,<span class="string">"Quit"</span>,<span class="string">"Quit Applications"</span>) </span><br><span class="line">        imp = wx.MenuItem(filemenu1, <span class="number">1</span>, <span class="string">"&amp;Import\tCtrl+I"</span>)</span><br><span class="line">obd = wx.MenuItem(filemenu1, <span class="number">2</span>, <span class="string">"&amp;Object\tCtrl+O"</span>)</span><br><span class="line">qui = wx.MenuItem(filemenu1, <span class="number">3</span>, <span class="string">"&amp;Quit\tCtrl+Q"</span>) </span><br><span class="line">        filemenu1.Append(imp)</span><br><span class="line">filemenu1.Append(obd)   </span><br><span class="line">filemenu1.Append(qui)</span><br><span class="line">        </span><br><span class="line">        self.Bind(wx.EVT_MENU, self.OnQuit, id=<span class="number">3</span>) </span><br><span class="line">        menuBar.Append(filemenu1, <span class="string">'&amp;File'</span>)</span><br><span class="line">...</span><br><span class="line">        </span><br><span class="line">     self.SetMenuBar(menuBar)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">OnQuit</span><span class="params">(self, e)</span>:</span> </span><br><span class="line">        self.Close()</span><br></pre></td></tr></table></figure><p>File-&gt;Quit (shortcut Ctrl+Q), “File” use wx.Menu() first to create menu filemenu, which defines a wx.MenuItem in this drop-down menu, self.Bind(wx.EVT MENU, self.OnQuit, id=3), event binder is used to associate the Quit function with the number 3 in the menu bar and the action to exit the window; In the drop-down menu Edit, the function Edit-&gt;Zoom in(Ctrl+’+’) /Zoom out(Ctrl+’- ‘) /Leftward /Righward /Upward /Downward respectively corresponds to the translational movement of the viewing angle in the direction of the front, back, left, and right, respectively. This is done through glTranslated(x, y, z) in the OpenGl.GL interface.</p><p><img src="/assets/dempost/dem2.png" alt="dem2"></p><p>Then create a toolbar. There are four buttons on the toolbar, which are the animation start, animation pause, next frame and previous frame. The corresponding button icon exists in the working directory /Inc.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">toolbar = self.frame.CreateToolBar()</span><br><span class="line">animation_tool = toolbar.AddTool(wx.ID_ANY,<span class="string">"Play"</span>,wx.Bitmap(<span class="string">"Inc/start.png"</span>)) </span><br><span class="line">suspend_tool = toolbar.AddTool(wx.ID_ANY,<span class="string">"Stop"</span>,wx.Bitmap(<span class="string">"Inc/stop.png"</span>))</span><br><span class="line">next_tool = toolbar.AddTool(wx.ID_ANY,<span class="string">"Next"</span>,wx.Bitmap(<span class="string">"Inc/next.png"</span>)) </span><br><span class="line">last_tool = toolbar.AddTool(wx.ID_ANY,<span class="string">"Last"</span>,wx.Bitmap(<span class="string">"Inc/last.png"</span>)) </span><br><span class="line">toolbar.Realize()</span><br><span class="line"></span><br><span class="line">self.Bind(wx.EVT_TOOL, self.OnClickStart, animation_tool) </span><br><span class="line">self.Bind(wx.EVT_TOOL, self.OnClickStop, suspend_tool) </span><br><span class="line">self.Bind(wx.EVT_TOOL, self.OnClickNext, next_tool) </span><br><span class="line">self.Bind(wx.EVT_TOOL, self.OnClickLast, last_tool)</span><br></pre></td></tr></table></figure><p>Each frame is a scene representing a certain moment, and a time series is added to obtain a time state stream. You can animate the scenes at each moment. The implementation method of this paper is to give all the time data in the scene class DEMPost, number each frame; then use the self.timer function to calculate time When the picture at this moment is updated, the modified frame number is passed to DEMPost, The data corresponding to this frame is filtered out and the picture is re-rendered; the pause of the animation is to keep the time unchanged, and the timer is turned off. At this time, the frame number is unchanged, and the scene does not change; the previous frame/next frame button is to pass the number of the previous/next frame to DEMPost and re-rendering.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">OnTimer</span><span class="params">(self, evt)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> self.timer.GetInterval()%<span class="number">500</span> == <span class="number">0</span>:</span><br><span class="line"><span class="keyword">try</span>: </span><br><span class="line">          self.canvas1.setValue(self.value) </span><br><span class="line">            self.value += <span class="number">1</span></span><br><span class="line"><span class="keyword">except</span> IndexError: </span><br><span class="line">          canvas1.setValue(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">OnClickStart</span><span class="params">(self, event)</span>:</span> </span><br><span class="line"> self.timer.Start(<span class="number">1000</span>) </span><br><span class="line">    self.canvas1.setInterac(<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">OnClickStop</span><span class="params">(self, event)</span>:</span> </span><br><span class="line">  self.timer.Stop() </span><br><span class="line">    self.canvas1.setInterac(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">OnClickNext</span><span class="params">(self, event)</span>:</span> </span><br><span class="line"><span class="keyword">if</span> self.timer.IsRunning():</span><br><span class="line">self.timer.Stop() </span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">self.value += <span class="number">1</span></span><br><span class="line">self.canvas1.setValue(self.value) </span><br><span class="line">    <span class="keyword">except</span> IndexError:</span><br><span class="line">canvas1.setValue(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">OnClickLast</span><span class="params">(self, event)</span>:</span> </span><br><span class="line">  <span class="keyword">if</span> self.timer.IsRunning():</span><br><span class="line">self.timer.Stop() </span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">self.value -= <span class="number">1</span></span><br><span class="line">self.canvas1.setValue(self.value) </span><br><span class="line">    <span class="keyword">except</span> IndexError:</span><br><span class="line">canvas1.setValue(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/assets/dempost/dem.gif&quot; alt=&quot;dem&quot;&gt;&lt;/p&gt;
&lt;h3 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;
      
    
    </summary>
    
    
    
      <category term="Visualization, DEM, OpenGL, UI" scheme="http://yoursite.com/tags/Visualization-DEM-OpenGL-UI/"/>
    
  </entry>
  
  <entry>
    <title>Texture map</title>
    <link href="http://yoursite.com/2019/10/08/Texture-map/"/>
    <id>http://yoursite.com/2019/10/08/Texture-map/</id>
    <published>2019-10-08T08:06:14.000Z</published>
    <updated>2019-11-08T10:21:08.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>An Ongoing project that transfer from multi-view image data to continuous rendering model.</p><p>Implement on <a href="http://humbi.cs.umn.edu" target="_blank" rel="noopener">HUMBI dataset</a> with Prof. Hyun Soo Park on UMN, which is not yet completed.</p><h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><ol><li>Generate <a href="https://smpl.is.tue.mpg.de" target="_blank" rel="noopener">SMPL</a> mesh reconstruction (realistic 3D model of the human body) from multi-view images</li></ol><p><img src="/assets/texture/tex1.png" alt="tex1"></p><ol start="2"><li><p>Generate IUV maps</p><p><img src="/assets/texture/tex2.png" alt="tex2"></p><p>x, y: the spatial coordinates of collected points on the image</p><p>I:    the patch index that indicates which of the 24 surface patches the point is on</p><p>UV:  coordinates in the UV space. Each surface patch has a separate 2D parameterization</p><p>I tried 2 approaches:</p></li></ol><ul><li><p>Mapping:</p><p>Use the UV map provided by <a href="http://densepose.org" target="_blank" rel="noopener">Densepose</a> to get UV coordinates for each vertices on the SMPL model. Then project UV coordinates on each view based on the camera parameters. However, it has to differentiate vertices on the front side and vertices on the back side, to get the projection.</p><p><img src="/assets/texture/tex3.png" alt="tex3"></p></li><li><p>Use <a href="http://densepose.org" target="_blank" rel="noopener">Densepose</a> to generate IUV map for each view images with multiview images directly.</p></li></ul><p><img src="/assets/texture/tex4.png" alt="tex4"></p><ol start="3"><li>Unwrap texture for each view for multi-view data</li></ol><p><img src="/assets/texture/tex5.png" alt="tex5"></p><ol start="4"><li>Integrate view-specific textures to form a complete texture of the subject</li></ol><p><img src="/assets/texture/tex6.png" alt="tex6"></p><ol start="5"><li><p>Make up for the gaps and artifacts of the texture</p></li><li><p>Wrap the texture on the SMPL mesh model</p></li></ol><h3 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h3><ol><li>Fill up gaps and generate missing parts in the texture</li><li>generate transition textures between two adjacent views</li></ol><p>Plan to finish the task by this semester.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h3&gt;&lt;p&gt;An Ongoing project that transf
      
    
    </summary>
    
    
    
      <category term="Multiview camera, 3D Reconstruction, texture" scheme="http://yoursite.com/tags/Multiview-camera-3D-Reconstruction-texture/"/>
    
  </entry>
  
  <entry>
    <title>Computer Graphics practice</title>
    <link href="http://yoursite.com/2018/11/02/Computer-Graphics/"/>
    <id>http://yoursite.com/2018/11/02/Computer-Graphics/</id>
    <published>2018-11-02T06:19:22.000Z</published>
    <updated>2019-11-14T05:57:19.000Z</updated>
    
    <content type="html"><![CDATA[<p>Assignments for course CSCi5607 at UMN with Prof. Stephen J. Guy.<br>HW1 - Graphics warmup, HW2 - Image Processing, HW3 - Ray tracing, HW0.5 - OpenGL startup, HW4 - 3D Game.</p><h3 id="HW4-3D-Game"><a href="#HW4-3D-Game" class="headerlink" title="HW4 - 3D Game"></a>HW4 - 3D Game</h3><p> <a href="https://youtu.be/9eI7D6LXkTo" target="_blank" rel="noopener">Demo</a></p><ul><li><em>Walls &amp; Doors:</em> Each map element type must have a unique rendering. Walls must look different than doors, and each of the five doors must look different than each other.</li></ul><ul><li><p><em>Keys:</em> Each key must be rendered as a physical object (e.g., a cube, teapot, or key model). Whenever a player moves the key must be rendered in front of them moving with the character.</p></li><li><p><em>User Input:</em> Users must be able to move around the map with mouse or keyboard input. One easy solution is to rotate with the left and right keys and move forward and back with the up and down keys.</p></li><li><p><em>Collision Detection:</em> Users must not be able to move through walls or locked doors.</p></li><li><p><em>Floors &amp; Ceilings:</em> Floors must be rendered, the ceiling is optional.</p></li><li><p><em>Lighting:</em> Your scene must have some ambient and diffuse lighting.</p></li></ul><p><img src="/assets/cg/cg1.png" alt="cg1"></p><p><img src="/assets/cg/cg2.png" alt="cg2"></p><p>​                      obtaining the key                                                 finding the door</p><ul><li>Texture map the walls and floors</li></ul><p><img src="/assets/cg/cg3.png" alt="cg3"></p><ul><li><p>Integrated keyboard and mouse control (must support strafing/sidestepping)<br>Up/Down key to step forward/backward<br>Left/Right for direction: turn left/right<br>‘A’ to grab objects(key)</p></li><li><p>Add characters to the environment which move on their own (perhaps chase the user slowly). These characters should not move through walls.</p></li></ul><p><img src="/assets/cg/cg4.png" alt="cg4"></p><p>Enemy is a Black and white plaid sphere, that move back and forth, game over when the player is attacked by the enemy.</p><ul><li><em>Character interaction:</em> Allow the user to freeze or shoot the other characters at a distance (you cannot freeze or shoot through walls!). press SPACE to strafe the enemy in a distance (with white sphere bullet) or use direction key for sidestepping</li><li>Animate the process of doors opening after they are unlocked.</li></ul><p><img src="/assets/cg/cg5.png" alt="cg5"></p><ul><li>Use models to represent the user, keys, doors, non-player characters etc. Extra points if you create these yourself (e.g., with <a href="http://www.wings3d.com/" target="_blank" rel="noopener">http://www.wings3d.com/</a>).</li></ul><p><img src="/assets/cg/cg6.png" alt="cg6"></p><ul><li>Load models in the OBJ 3d models format.<br>see model/mykey.txt which is transformed from an obj model built with wings3d.</li></ul><p><img src="/assets/cg/cg7.png" alt="cg7"></p><p>the colorful sphere is the goal</p><h3 id="HW0-5-OpenGL-startup"><a href="#HW0-5-OpenGL-startup" class="headerlink" title="HW0.5 - OpenGL startup"></a>HW0.5 - OpenGL startup</h3><ul><li><em>SDL_CreateWindow</em></li><li>We made use of a depth buffer to prevent faces rendering to the front while they’re behind other faces. When depth testing is  enabled. OpenGL tests the depth value of a fragment against the content of the depth buffer.  OpenGL performs a depth test and if this test passes, the depth buffer is updated with the new depth value. If depth test falls, the fragment is discarded.</li></ul><p><img src="/assets/cg/cg8.gif" alt="cg8">)<img src="/assets/cg/cg9.gif" alt="cg9"></p><p>pressing up/down on the keyboard</p><p>Teapot rendering, Phong lighting model</p><ul><li>translate the model by pressing up/down/right/left on the keyboard, speed up the rotation by pressing “R”.</li></ul><p><img src="/assets/cg/cg10.gif" alt="cg10">)<img src="/assets/cg/cg11.gif" alt="cg11"></p><ul><li><em>Blinn–Phong reflection model</em>, brighter</li></ul><p><img src="/assets/cg/cg12.gif" alt="cg12"></p><ul><li>press ‘c’ to change color randomly</li></ul><p><img src="/assets/cg/cg13.gif" alt="cg13"></p><ul><li>press “L“ to rotate the direction of light</li></ul><p><img src="/assets/cg/cg14.gif" alt="cg14"></p><ul><li>Use a point light located at (-1.5, 0, 0).</li></ul><p><img src="/assets/cg/cg15.gif" alt="cg15"></p><ul><li>gouraud rendering, hardly any rendering quality loss, and it is faster</li></ul><p><img src="/assets/cg/cg16.gif" alt="cg16"></p><h3 id="HW3-Ray-tracing"><a href="#HW3-Ray-tracing" class="headerlink" title="HW3 - Ray tracing"></a>HW3 - Ray tracing</h3><ul><li>Basic features:</li></ul><p>Arbitrary camera placement, film resolution, and aspect ratio<br>Arbitrary scenes with spheres, triangles (possible with vertex normals), and arbitrary background colors<br>Color &amp; Specularity (Phong Lighting Model)<br>Arbitrary materials, including diffuse and specular shading, reflections, and refractions<br>Ambient lighting, Point and directional lights<br>Shadows<br>Recursion to a bounded depth<br>Difference/Intersection of spheres (Constructive Solid Geometry)<br>Triangles<br>BMP or PNG output</p><p><img src="/assets/cg/cg17.jpg" alt="cg17">)<img src="/assets/cg/cg18.jpg" alt="cg18">)!<img src="/assets/cg/cg19.jpg" alt="cg19">)<img src="/assets/cg/cg20.jpg" alt="cg20">)<img src="/assets/cg/cg21.jpg" alt="cg21"></p><ul><li>Additional features:</li></ul><ol><li><p>An acceleration structure: Bounding Volume Hierarchy (BVH).<br>See aabb.h, put sphere and triangle objects into box(aabb), intersect with the object when the box is hit by the ray. Then build a bvh tree, see bvhnode.h.<br>result from no bvh to using bvh. Use line331 in main.cpp and instead of line330, then rebuild the project.<br>-pic (a): 3s -&gt; 7s<br>-pic (b): 6s -&gt; 18s<br>-pic (c): 4s -&gt; 4s<br>-pic (d): approximately 1h -&gt; 178s<br>We can see that in a simple scene with a few objects, especially evenly distributed scene, bvh structure work worse. Because it spend more time building the hierarchy structure. However, in a complex scene like the dragon, bvh structure is much more efficient.</p></li><li><p>Parallelize the raytracer with OpenMP<br>Include omp.h, see line 336 in main.cpp. , #pragma omp parallel for. Time for rendering pic(d) is reduced to 47s.</p></li><li><p>Jittered supersampling<br>See line 340 main.cpp, multiple samples per pixel, adding random noise each sample.<br>sn is the number of samples per pixel. sn=1 denotes uniform supersampling. Image becomes smoother when sn is bigger, but it may overkill simple area of the image; also, it will multiply the time.</p></li><li><p>User interface that shows the raytraced image being updated<br>implemeneted in testui.py. Run <em>python testui.py</em> on command line after building the raytracer execuatble.</p></li></ol><p><img src="/assets/cg/cg22.png" alt="cg22"></p><ol start="5"><li>Boxes and Planes</li></ol><p><img src="/assets/cg/cg23.jpg" alt="cg23"></p><h3 id="HW2-Image-Processing"><a href="#HW2-Image-Processing" class="headerlink" title="HW2 - Image Processing"></a>HW2 - Image Processing</h3><ol><li>Contrast: Change the contrast and saturation: Change the saturation of an image by factor 0.7, 1.1, 1.4.</li></ol><p><img src="/assets/cg/cg28.png" alt="cg28"></p><ol start="2"><li>Quantize: Change the number of bits per channel of an image, using simple rounding, nbits=6, 4, 2.</li></ol><p><img src="/assets/cg/cg29.png" alt="cg29"></p><ol start="3"><li>Random dither: Convert an image to a given number of bits per channel, using a randomized threshold, nbits=4, 2, 1.</li></ol><p><img src="/assets/cg/cg30.png" alt="cg30"></p><p>​        Floyd-Steinberg dither: Convert an image to a given number of bits per channel, using dithering with error diffusion, nbits=4, 2, 1.</p><p><img src="/assets/cg/cg31.png" alt="cg31"></p><p>​        Ordered dither: Convert an image to a given number of bits per channel, nbits=4, 2, 1.</p><p><img src="/assets/cg/cg32.png" alt="cg32"></p><ol start="4"><li><p>Scale: Scale an image up or down in size by a real valued factor with inverse wrapping, and rotate<br><img src="/assets/cg/cg35.png" alt="cg35"><br>using point sampling, bilinear sampling, and Gaussian sampling<br><img src="/assets/cg/cg36.png" alt="cg36"></p></li><li><p>Additional: Warp an image using a non-linear mapping</p></li></ol><p><img src="/assets/cg/cg38.png" alt="cg38"></p><h3 id="HW1-Graphics-Warmup"><a href="#HW1-Graphics-Warmup" class="headerlink" title="HW1 - Graphics Warmup"></a>HW1 - Graphics Warmup</h3><p><img src="/assets/cg/cg40.gif" alt="cg40">)<img src="/assets/cg/cg41.gif" alt="cg41">)<img src="/assets/cg/cg42.gif" alt="cg42">)<img src="/assets/cg/cg43.gif" alt="cg43"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Assignments for course CSCi5607 at UMN with Prof. Stephen J. Guy.&lt;br&gt;HW1 - Graphics warmup, HW2 - Image Processing, HW3 - Ray tracing, HW
      
    
    </summary>
    
    
    
      <category term="Computer Graphics, rendering, ray tracing, image processing" scheme="http://yoursite.com/tags/Computer-Graphics-rendering-ray-tracing-image-processing/"/>
    
  </entry>
  
  <entry>
    <title>Dear Data</title>
    <link href="http://yoursite.com/2018/10/14/Dear-Data/"/>
    <id>http://yoursite.com/2018/10/14/Dear-Data/</id>
    <published>2018-10-14T05:43:34.000Z</published>
    <updated>2019-11-14T08:01:15.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Weekly “visual creativity enhancing” exercise, making us think creatively about new ways of mapping data to visuals and quickly prototype them. If they seem to work well, then we might later program a visualization to use the same visual style. What’s more, the process of collecting our own data will make us think about and develop a more “humanistic” approach to working with data. There is always a human somewhere behind the data, and, as a discipline, we need to become more aware of this throughout the entire data science pipeline.</p><p>To know more about the “Dear Data” project: <a href="https://www.ted.com/talks/giorgia_lupi_how_we_can_find_ourselves_in_data#t-225912" target="_blank" rel="noopener">Georgia Lupi’s Ted Talk</a>.</p><ol><li><p>Collect Some Data During the Week</p></li><li><p>Design a Creative Mapping from Data to Visual</p></li><li><p>Create a Postcard-Sized Visualization with Non-Traditional Visualization Media</p></li></ol><h3 id="My-works"><a href="#My-works" class="headerlink" title="My works"></a>My works</h3><p><img src="/assets/deardata/dd1.png" alt="dd1"></p><p><img src="/assets/deardata/dd2.png" alt="dd2"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h3&gt;&lt;p&gt;Weekly “visual creativity enha
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
